<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1dl..1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title>ICCV 2015 tutorial on probabilistic deep learning</title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">main</div>
<div class="menu-item"><a href="dl.html#home">home</a></div>
<div class="menu-item"><a href="dl.html#description">description</a></div>
<div class="menu-item"><a href="dl.html#organizers">organizers</a></div>
<div class="menu-category">program</div>
<div class="menu-item"><a href="program.html#schedule">schedule</a></div>
</td>
<td id="layout-content">
<div id="toptitle">
<h1>ICCV 2015 Tutorial: From Image Statistics to Deep Learning <a name="home"></a></h1>
</div>
<div class="infoblock">
<div class="blockcontent">
<p>Full-day tutorial in conjunction with <a href="http://www.pamitc.org/iccv15/">ICCV 2015</a>.<br />
Friday, December 11, 2015, Santiago, Chile<br /></p>
</div></div>
The tutorial's program and slides are available <a href="program.html">here</a>.</p>
<h2>Description <a name="description"></a></h2>
<p>
Over the last few years, deep learning has delivered state-of-the-art results in a host of problems in computer vision, and machine learning in general.  Even though the basic techniques e.g. for classification with deep convolutional networks are fairly straightforward (loss minimization through back-propagation), the wealth of the broader techniques that pertain to probabilistic deep modeling may not be as accessible in a first encounter.    	</p>The goal of this tutorial is to elaborate on the ties between probabilistic approaches to deep learning and traditional methods from statistical signal processing and statistical data modeling. Our aim is to provide the background necessary to make a larger part of probabilistic deep learning accessible to starting graduate students with an electrical engineering and/or computer science background.  	</p>In particular, flat, single-layer probabilistic models have been extensively studied in image processing, computer vision and machine learning, and in several cases come equipped with exact methods for inference and learning; 	for instance linear models with or without sparsity constraints often lead to analytic Maximum-a-Posteriori solutions, while Maximum Entropy parameter estimation for the exponential family leads to convex optimization problems with unique solutions. As such, understanding such models is easier and provides a solid background on which to develop intuition for deeper and less tractable models.  </p>For this, we will approach deep generative models from a statistical image modeling perspective, starting from linear models, moving on to Boltzmann-Gibbs distributions, Markov Random Fields, and Restricted Boltzmann Machines, and eventually will cover Deep Belief Networks and Deep Boltzmann Machines. We will finally present non-probabilistic deep relaxations such as convolutional neural networks and stacked auto-encoders while exploiting the intuitions developed from probabilistic modeling. 
</p>
<h2>Organizers <a name="organizers"></a></h2>
<ul>
<li><p><a href="http://cvn.ecp.fr/personnel/iasonas/">Iasonas Kokkinos</a>, CentraleSupelec/INRIA</p>
</li>
<li><p><a href="http://ttic.uchicago.edu/~gpapan">George Papandreou</a>, Google Research</p>
</li>
</ul>

<div id="footer">
<div id="footer-text">
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
